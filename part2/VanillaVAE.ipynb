{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoder (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def\\abs#1{\\left\\lvert #1 \\right\\rvert} \\def\\Set#1{\\left\\{ #1 \\right\\}} \\def\\mc#1{\\mathcal{#1}} \\def\\M#1{\\boldsymbol{#1}} \\def\\R#1{\\mathsf{#1}} \\def\\RM#1{\\boldsymbol{\\mathsf{#1}}} \\def\\op#1{\\operatorname{#1}} \\def\\E{\\op{E}} \\def\\d{\\mathrm{\\mathstrut d}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- ~~explanation for the trainning and ploting~~\n",
    "- the analysis of the two term in the ELBO and relation with Information theory\n",
    "- ~~infoVAE~~\n",
    "- Examples showing why InfoVAE\n",
    "- add references in relevant part\n",
    "\n",
    "Notations:  \n",
    "*When substitue, remember skip the code*\n",
    "- ~~substitue Z by \\R{Z} and similar for X, J . but need skip \\mc{X}, \\mc{Z}. *DO NOT DIRECTLY REPLACE IN VS CODE, BETTER DO THIS WITH JSON VIEW* \n",
    "Several place involve \\R{z} to indicate an element of Z as random variable, which has already used \\R{z}, same for \\R{\\hat{z}}~~\n",
    "- ~~substitue Encoder by *Encoder*, and similar for decoder~~\n",
    "\n",
    "To improve:\n",
    "- why directly sampling posterior is intractable\n",
    "- reparameterization trick, seems have relationship with importance sampling\n",
    "- whether also use sampling for x_hat in the experiment\n",
    "- where to put the code for importing packages\n",
    "- should we use capital variable name for batch_size, epochs, etc..\n",
    "- ~~The train dateset and test dataset maybe combined. Currently, use the train dataset for training, and test dataset for evaluating is a bit improper to see the reconstruction performance.~~  Only use train_dataset for visuallization\n",
    "\n",
    "- the Modeling bias problem in VAE, may use     $L_{\\text{VAE}} = D(P_{X|\\hat{Z}} || P_{\\hat{X}|Z}) + D(P_{\\hat{Z}} || P_{Z}) +H(X)$ , but may not metion this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark  \n",
    "- The following follows the idea from https://arxiv.org/abs/1606.05908 , Tutorial on Variational Autoencoders   \n",
    "\n",
    "- I skipped the illustration from $p_{\\hat{X}|Z}(x|z) = \\mc{N}(\\mu_{\\phi}, \\sigma^2 * \\M{I})$ to $p_{\\hat{X}|Z}(x|z) = \\mc{N}(\\mu_{\\phi}, \\Sigma_{\\phi})$ \n",
    "\n",
    "- About the notation, since the standard Gaussian distribution in latent space is introduced first, ***$Z$ is the know standard Gaussian distribution, and then $\\hat{Z}$ is the representation given by the encoder***. the pairs are: $X - (Encoder) - \\hat{Z}$, $Z - (Decoder) - \\hat{X}$, $\\hat{Z} - (Decoder) - \\bar{X}$.  \n",
    "When we consider VIB, we may need the markov chain $Y - X - Z$, where $Z$ is used as the representation of $X$, and if VIB is involved, then introduce $\\bar{Z}$ as the know stardard Gaussian distribution is better, and the pairs are: $X - (Encoder) - Z$, $\\bar{Z} - (Decoder) - \\hat{X}$, $Z - (Decoder) - \\bar{X}$.  \n",
    "If switch $Z$ and $\\hat{Z}$ is needed, remember do this for little z and \\hat{z}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset containing $N$ samples from random variable $\\R{X}$.  \n",
    "How to design a model that with latent random variariable $\\R{\\R{Z}}$ as input, it generate different samples that are like those already in a trainning datase, but not exactly the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a deterministic function $g: \\mc{\\R{Z}} \\rightarrow \\mc{\\R{X}}$ parametrized by $\\phi$ which models $p_{\\hat{\\R{X}}|\\R{Z}}$, i.e., outputs the generated samples with input from latent space $\\mc{\\R{Z}}$.  \n",
    "\n",
    "When $\\R{Z}$ is random, $g(\\R{Z})$ will be a random variable in sample space $\\mc{\\R{X}}$. \n",
    "e want to choose the $\\phi$ such that by sampling a $z \\sim p_{\\R{Z}}$, $g(z)$ will be like the $x's$ in the dataset with high probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may try to maximize the probability for generating sample $\\hat{\\R{X}}$ by training the the parameter $\\phi$ of function $g$ according to the law of total probability:\n",
    "$$p_{\\hat{\\R{X}}}(\\R{X})=\\int p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|z) p_{\\R{Z}}(z) dz.$$ \n",
    "\n",
    "We assume $p_{\\R{Z}}$ is a standard Gaussian distribution.  \n",
    "For $p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|z)$, usually we can assume it to be a multivariate Gaussian or Bernoulli distribution. Here we assume it to be a Gaussian distribution with mean and variance given by $g(\\R{Z})$. This is to say,   \n",
    "$$\\R{Z} \\sim \\mc{N}(\\M{0},\\M{I}),$$ \n",
    "and $$p_{\\hat{\\R{X}}|\\R{Z}}(x|z) = \\mc{N}(x | \\mu_{\\phi}, \\Sigma_{\\phi}),$$ \n",
    "where \n",
    "$$[\\mu_{\\phi}, \\Sigma_{\\phi}] = g(z).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empiracal way to approximate $p_{\\hat{\\R{X}}}$ is to sammple a large number of $z$ values $\\{z_1, z_2, \\dots, z_n\\}$ from $\\R{Z}$ followed by computing:\n",
    "$$p_{\\hat{\\R{X}}}(\\R{X}) \\approx \\frac{1}{n} \\sum_{i} p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|z_i)$$\n",
    "\n",
    "However, this needs too many samples and is intractable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to sample the $z$ values that are likely to generate $x$. This means we need inferring $\\hat{\\R{Z}}$ from $\\R{X}$.\n",
    "\n",
    "We assume $p_{\\hat{\\R{Z}}|\\R{X}}(\\R{Z}|x)$ is a Gaussian distribution, and use a trainable function $f:\\mc{\\R{X}} \\rightarrow \\mc{\\R{Z}}$ parameterized by $\\theta$ to output the mean and variance of $p_{\\hat{\\R{Z}}|\\R{X}}(\\R{Z}|x)$. This is to say\n",
    "$$ p_{\\hat{\\R{Z}}|\\R{X}}(z|x) =\\mc{N}(z | \\mu_{\\theta}, \\Sigma_{\\theta}), $$\n",
    "where \n",
    "$$[\\mu_{\\theta}, \\Sigma_{\\theta}] = f(x).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have: \n",
    "$$ p_{\\hat{\\R{X}}}(x) = \\underbrace{E[p_{\\hat{\\R{X}}|\\R{Z}}(x|\\hat{\\R{Z}})| x=\\R{X}] -D(P_{\\hat{\\R{Z}}|\\R{X}}(\\cdot | x) || P_{\\R{Z}})}_{\\text{ELBO}(x)} + D(P_{\\hat{\\R{Z}}|\\R{X}}(\\cdot | x) | P_{\\R{Z}|\\R{X}}(\\cdot | x)).$$\n",
    "\n",
    "Hence,\n",
    "$$ E[p_{\\hat{\\R{X}}}(\\R{X})] \\geq \\underbrace{E[p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|\\hat{\\R{Z}})] - D(P_{\\hat{\\R{Z}}|\\R{X}} || P_{\\R{Z}}|P_{\\R{X}})}_{\\text{ELBO}}. $$\n",
    "\n",
    "Then to maximize $E[p_{\\hat{\\R{X}}}(\\R{X})]$, we can optimize the parameters of the whole model by maximizing the ELBO.  \n",
    "\n",
    "We usually minimizing an objective function, and it is\n",
    "$$\\min_\\limits{\\phi, \\theta} L_{\\text{VAE}} := E[ - p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|\\hat{\\R{Z}})] + D(P_{\\hat{\\R{Z}}|\\R{X}} || P_{\\R{Z}}|P_{\\R{X}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise** \n",
    "\n",
    "Prove that \n",
    "$$ p_{\\hat{\\R{X}}}(x) \\geq E[p_{\\hat{\\R{X}}|\\R{Z}}(x|\\hat{\\R{Z}})|x=\\R{X}] -D(P_{\\hat{\\R{Z}}|\\R{X}}(\\cdot | x) || P_{\\R{Z}}). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use neural networks for $f$ and $g$, and we call $f$ the $Decoder$ and $g$ the *Encoder*, since the structure of the whole model resembles an autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference  \n",
    "https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/01_Variational_AutoEncoder.ipynb\n",
    "https://github.com/AntixK/PyTorch-VAE\n",
    "https://github.com/GuHongyang/VaDE-pytorch\n",
    "\n",
    "Tutorial on Variational Autoencoders  https://arxiv.org/abs/1606.05908   \n",
    "\n",
    "VAE https://arxiv.org/abs/1312.6114   \n",
    "Fixing a broken ELBO http://proceedings.mlr.press/v80/alemi18a.html   \n",
    "beta-vae: Learning basic visual concepts with a constrained variational framework https://openreview.net/forum?id=Sy2fzU9gl   \n",
    "Understanding disentangling in Î²-VAE https://arxiv.org/abs/1804.03599    \n",
    "Isolating Sources of Disentanglement in VAEs https://arxiv.org/abs/1802.04942   \n",
    "InfoVAE https://ojs.aaai.org/index.php/AAAI/article/view/4538   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "\n",
    "dataset_path = './datasets'\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "x_dim  = 784 # dimension of input data for Decoder\n",
    "latent_dim = 200 # dimension for latent representation z\n",
    "hidden_dims = [400] \n",
    "decoder_hidden_dims = None # if not None, it means specifying different hidden nodes of internal layers for Decoder with Encoder\n",
    "\n",
    "beta = 1 # the coefficient for the divergence term in L_{VAE}, beta = 1 is the vanilla VAE\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use MNIST dataset as an example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the *Encoder*, i.e., the function $f$, for inferring $\\hat{\\R{Z}}$ from $\\R{X}$.\n",
    "$$ p_{\\hat{\\R{Z}}|\\R{X}}(z|x) =\\mc{N}(z | \\mu_{\\theta}, \\Sigma_{\\theta}), $$\n",
    "where \n",
    "$$[\\mu_{\\theta}, \\Sigma_{\\theta}] = f(x).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume $\\Sigma_{\\theta}$ is diagonal, i.e.,\n",
    "$$\\Sigma_{\\theta} := \\text{diag}\\left(\\left\\{\\sigma_{\\theta, 1}^2, \\sigma_{\\theta, 2}^2, \\dots,  \\sigma_{\\theta, d_{\\R{Z}}}^2 \\right\\}\\right) =  \\text{diag} \\left( \\left\\{\\sigma_{\\theta, j}^2 \\right\\}_{j=1}^{d_{\\R{Z}}} \\right),$$ \n",
    "where $d_{\\R{Z}}$ is the dimension of $\\R{Z}$.  \n",
    "\n",
    "The output of the *Encoder* is $\\mu_{\\theta}$ and $\\left[\\log \\sigma_{\\theta, 1}^2, \\log \\sigma_{\\theta, 2}^2, \\dots,  \\log \\sigma_{\\theta, d_{\\R{Z}}}^2 \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, x_dim=x_dim, latent_dim=latent_dim, hidden_dims = hidden_dims):\n",
    "        super(Encoder,self).__init__()\n",
    " \n",
    "        self.x_dim = x_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        modules = []\n",
    "        \n",
    "        self.input_dim = x_dim\n",
    "        \n",
    "        for h_dim in self.hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(self.input_dim, h_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "            self.input_dim = h_dim\n",
    "        \n",
    "        self.encoder=nn.Sequential(*modules)\n",
    "        \n",
    "        self.mu_z=nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.log_var_z=nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        e=self.encoder(x)\n",
    "        \n",
    "        mu=self.mu_z(e)\n",
    "        log_var=self.log_var_z(e)\n",
    "\n",
    "        return mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the *Decoder*, i.e., the function $g$, for generating $\\hat{\\R{X}}$ from $\\R{Z}$.  \n",
    "\n",
    "$$p_{\\hat{\\R{X}}|\\R{Z}}(x|z) = \\mc{N}(x | \\mu_{\\phi}, \\Sigma_{\\phi}),$$ \n",
    "where \n",
    "$$[\\mu_{\\phi}, \\Sigma_{\\phi}] = g(z).$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default hidden dimensions of the layers in *Decoder* are the reverse order of those in *Encoder*.  \n",
    "\n",
    "We can also define a different one for *Decoder* by setting the argument *decoder_hidden_dims*.  \n",
    "\n",
    "In the Decoder here, for an input $z$ we use $\\mu_{\\phi}$ as $\\hat{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, x_dim=x_dim, latent_dim=latent_dim, hidden_dims = hidden_dims, decoder_hidden_dims = decoder_hidden_dims):\n",
    "        super(Decoder,self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.decoder_hidden_dims = decoder_hidden_dims\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        if self.decoder_hidden_dims is None:\n",
    "            self.decoder_hidden_dims = list(reversed(self.hidden_dims))\n",
    "\n",
    "        self.input_dim = self.latent_dim\n",
    "\n",
    "        for h_dim in self.decoder_hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(self.input_dim, h_dim),\n",
    "                    nn.ReLU(inplace=True)\n",
    "                )\n",
    "            )\n",
    "            self.input_dim = h_dim\n",
    "\n",
    "        self.decoder=nn.Sequential(*modules)\n",
    "\n",
    "        self.mu_x=nn.Linear(self.decoder_hidden_dims[-1], self.x_dim)\n",
    "        # self.log_var_x=nn.Linear(hidden_dims[-1], self.x_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        d=self.decoder(z)\n",
    "        \n",
    "        # use Sigmoid activation to ensure elements of mu, or the x_hat below are in (0,1)\n",
    "        mu=torch.sigmoid(self.mu_x(d))\n",
    "        # log_var=self.log_var_x(d)\n",
    "\n",
    "        # for decoder, we care about the mean\n",
    "        x_hat = mu\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reparameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the *Encoder* and *Decoder*, we need the gradients with respect to the trainable parameters, and backpropagate through the neural networks.  \n",
    "\n",
    "However, the input for the *Decoder*, $\\hat{z}$ values is randomly sampled from $\\mc{N}(\\mu_{\\theta}, \\Sigma_{\\theta})$, and the backpropagation cannot flow through such a random node.  \n",
    "\n",
    "Instead of directly sampling $\\hat{z}$ values the reparameterization trick introduces a random variable $$\\epsilon \\sim \\mc{N}(\\M{0},\\M{I}),$$ and sample \n",
    "$$\\epsilon_i \\sim \\mc{N}(\\M{0},\\M{I}),$$ then $z_i$ is obtained by \n",
    "$$z_i = \\mu_{\\theta} + \\text{diag} \\left( \\left\\{\\sigma_{\\theta, j} \\right\\}_{j=1}^{d_{\\R{Z}}} \\right) \\cdot \\epsilon_i,$$ \n",
    "where we assume $\\Sigma_{\\theta}$ is a diagonal matrix with \n",
    "$$\\Sigma_{\\theta} := \\text{diag} \\left( \\left\\{\\sigma_{\\theta, j}^2 \\right\\}_{j=1}^{d_{\\R{Z}}} \\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterization(mean, std):\n",
    "    epsilon = torch.randn_like(std).to(DEVICE)        # sampling epsilon        \n",
    "    z = mean + std*epsilon                          # reparameterization trick\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model composed of *Encoder* and *Decoder*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = reparameterization(mu, torch.exp(0.5 * log_var))\n",
    "        x_hat = self.decoder(z)\n",
    "        \n",
    "        return x_hat, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder()\n",
    "decoder=Decoder()\n",
    "\n",
    "model = Model(encoder=encoder, decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def loss_function(x, x_hat, mu, log_var):\n",
    "    reconstruction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='mean')\n",
    "    KLD = - 0.5 * torch.mean(1+ log_var - mu ** 2 - log_var.exp())\n",
    "\n",
    "    return reconstruction_loss + beta * KLD, reconstruction_loss, KLD \n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Start training VAE...\")\n",
    "model.train()\n",
    "\n",
    "loss_list = []\n",
    "reconstruction_loss_list = []\n",
    "KLD_list = []\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    overall_reconstruction_loss = 0\n",
    "    overall_KLD = 0\n",
    "    overall_sample_number = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "\n",
    "        x = x.view(-1, x_dim)\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mu, log_var = model(x)\n",
    "        loss, reconstruction_loss, KLD = loss_function(x, x_hat, mu, log_var)\n",
    "        \n",
    "        overall_loss += loss.item() * x.shape[0]\n",
    "        overall_reconstruction_loss += reconstruction_loss.item() * x.shape[0]\n",
    "        overall_KLD += KLD.item() * x.shape[0]\n",
    "        overall_sample_number += x.shape[0]\n",
    "\n",
    "        train_loss = overall_loss / overall_sample_number\n",
    "\n",
    "        train_reconstruction_loss = overall_reconstruction_loss / overall_sample_number\n",
    "        train_KLD = overall_KLD / overall_sample_number\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_list.append(train_loss)\n",
    "    reconstruction_loss_list.append(train_reconstruction_loss)\n",
    "    KLD_list.append(train_KLD)\n",
    "\n",
    "    # print(datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S'), \"\\tEpoch\", epoch + 1, \":\", \"\\tKLD Loss: \", overall_KLD / overall_sample_number, \"\\treconstruction Loss: \", overall_reconstruction_loss / overall_sample_number, \"\\tLoss: \", overall_loss / overall_sample_number)\n",
    "\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S'), \" \\tEpoch {}: \\tKLD Loss: {:.6f} \\treconstruction Loss: {:.6f} \\tLoss: {:.6f}\".format(epoch + 1, train_KLD, train_reconstruction_loss, train_loss))\n",
    "\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1,epochs+1), loss_list, label = 'loss')\n",
    "plt.plot(range(1,epochs+1), reconstruction_loss_list, label = 'reconstruction_loss')\n",
    "plt.plot(range(1,epochs+1), KLD_list, label = 'KLD')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visuallization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can input a integer in the *text box*, or drag the *slider bar*, and the value will be the index for the sample which will be shown with title \"orignal image\", and the corresponding reconstructed image will also be shwon.  \n",
    "\n",
    "You might see some images are not reconstructed well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_dataset = train_dataset # select the dataset to visualize, values: train_dataset, test_dataset\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def plot_x(i):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x=selected_dataset[i][0]\n",
    "        x = x.view(-1, x_dim)\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        x_hat, _, _ = model(x)\n",
    "\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title('Original image')\n",
    "        plt.imshow(x.view(28,28).cpu().numpy())\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title('reconstructed image')\n",
    "        plt.imshow(x_hat[0].view(28,28).cpu().numpy())\n",
    "        # plt.imshow(x_hat[0].view(28,28).cpu().numpy(), cmap=\"gray\")\n",
    "        plt.show()\n",
    "\n",
    "widget_a = widgets.BoundedIntText(value=1, min=0, max=len(selected_dataset)-1, step=1)\n",
    "widget_b = widgets.IntSlider(value=1, min=0, max=len(selected_dataset)-1, step=1)\n",
    "mylink = widgets.jslink((widget_a, 'value'), (widget_b, 'value'))\n",
    "\n",
    "display(widget_a)\n",
    "interact(plot_x, i = widget_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate samples by randomly sampling $z$ values from $p_{\\R{Z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(batch_size, latent_dim).to(DEVICE)\n",
    "    generated_images = decoder(noise)\n",
    "\n",
    "save_image(generated_images.view(-1, 1, 28, 28), 'generated_samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename =r\"./generated_samples.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the single generated image by changing the value of the argument *idx*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(x, idx, title=None):\n",
    "    x = x.view(batch_size, 28, 28)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    if title is not None:\n",
    "        plt.suptitle(title + ', idx=' + str(idx))\n",
    "    plt.imshow(x[idx].cpu().numpy())\n",
    "\n",
    "show_image(generated_images, idx=0, title='generated image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Exercise** \n",
    "\n",
    "What is the relationship between the divergence term in $L_{\\text{VAE}}$ and $I(\\hat{\\R{Z}} \\wedge \\R{X})$?  \n",
    "\n",
    "What will happen if the model is sufficiently flexible and the $L_{\\text{VAE}}$ achieves optimal value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try use stronger *Encoder* and *Decoder*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the two terms in $L_{\\text{VAE}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss $L_{\\beta}$\n",
    "$$L_{\\beta} := E[ - p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|\\hat{\\R{Z}})] + \\beta D(P_{\\hat{\\R{Z}}|\\R{X}} || P_{\\R{Z}}|P_{\\R{X}}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distanglement of latent representation in $\\beta$-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a uniform random variable $\\R{J}$ on $\\{1,2,\\dots, N\\}$ which relates to the samples in the dataset. Then $x_{\\R{J}}$ is a random variable.  \n",
    "Define\n",
    "$$ p_{\\hat{\\R{Z}}|x_{\\R{J}}} (\\cdot|x_j) := p_{\\hat{\\R{Z}}|\\R{J}}(\\cdot|j). $$\n",
    "Then we have\n",
    "\\begin{align*}\n",
    "    p_{\\R{J}}(j) &= \\frac{1}{N}, \\\\\n",
    "    p_{\\hat{\\R{Z}}, \\R{J}}(\\cdot|j) &= p_{\\hat{\\R{Z}} | \\R{J}}(\\cdot|j) p_{\\R{J}}(j) = \\frac{1}{N}p_{\\hat{\\R{Z}} | \\R{J}}(\\cdot|j) \n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then \n",
    "\\begin{align*}\n",
    "D(P_{\\hat{\\R{Z}}|\\R{X}} || P_{\\R{Z}}|P_{\\R{X}}) &= E \\left[ D(P_{\\hat{\\R{Z}}|\\R{X}})(\\cdot|\\R{X}) || P_{\\R{Z}} \\right] \\\\\n",
    "&= E \\left[ D(P_{\\hat{\\R{Z}}|\\R{J}})(\\cdot|\\R{J}) || P_{\\R{Z}} \\right] \\\\\n",
    "&= \\underbrace{D \\left( P_{\\hat{\\R{Z}} \\R{J}} || P_{\\hat{\\R{Z}}} P_{\\R{J}} \\right)}_{\\text{Index-Code MI}} \n",
    "+ \\underbrace{D \\left( P_{\\hat{\\R{Z}}}|| \\prod_{k=1}^{d_{\\R{Z}}} P_{\\R{z}_k} \\right)}_{\\text{Total Correlation}} \n",
    "+ \\underbrace{ \\sum_{k=1}^{d_{\\R{Z}}} D \\left( P_{\\hat{\\R{z}}_k} || P_{\\R{z}_k} \\right)}_{\\text{Dimension-wise divergence}}.\n",
    "\\end{align*}  \n",
    "For details, read [Isolating Sources of Disentanglement in VAEs](https://arxiv.org/abs/1802.04942)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two problems of $L_{\\text{VAE}}$:\n",
    "- When model capacity is limited, there is a tradoff between the two terms in $L_{\\text{VAE}}$, and the variational posterior $p_{\\hat{\\R{Z}}|\\R{X}}$ may be quit inaccurate.  \n",
    "    - The reconstruction term can encorage choosing $p_{\\hat{\\R{Z}}|\\R{X}}(\\cdot|x_i)$ and $p_{\\hat{\\R{Z}}|\\R{X}}(\\cdot|x_j)$ to choose disjoint support when $x_i \\not= x_j$. The divergence term in $L_{\\text{VAE}}$ may not always sufficient to prevent this.\n",
    "\n",
    "- When model capacity is sufficiently large, $I(\\hat{\\R{Z}} \\wedge \\R{X})$ tends to be vanishingly small, and $\\hat{\\R{X}}$ tend to be independent of $\\R{Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L_{\\text{VAE}} = D(P_{X|\\hat{Z}} || P_{\\hat{X}|Z}) + D(P_{\\hat{Z}} || P_{Z}) + \\underbrace{H(\\R{X})}_{\\text{constant}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: add a term in the loss to encourage the MI between $\\hat{\\R{Z}}$ and $\\R{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The infoVAE loss:\n",
    "$$\n",
    "\\begin{align}\n",
    "L_{InfoVAE} &= D(P_{\\R{X}|\\hat{\\R{Z}}} || P_{\\hat{\\R{X}}|\\R{Z}}|P_{\\hat{\\R{Z}}}) + \\gamma D(P_{\\hat{\\R{Z}}}||P_{\\R{Z}}) - \\alpha I(\\hat{\\R{Z}} \\wedge \\R{X}) \\\\\n",
    "&= E[\\log p_{\\R{X}|\\hat{\\R{Z}}}(\\R{X}|\\hat{\\R{Z}})] + E[- \\log p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|\\hat{\\R{Z}})] + \\gamma E[\\log p_{\\hat{\\R{Z}}}(\\hat{\\R{Z}})]- \\gamma E [\\log p_{\\R{Z}}(\\hat{\\R{Z}})]  - \\alpha E[\\log p_{\\hat{\\R{Z}}|\\R{X}}(\\hat{\\R{Z}}|\\R{X})] + \\alpha E[\\log_{\\hat{\\R{Z}}}(\\hat{\\R{Z}})]  \\\\\n",
    "&= E[\\log p_{\\hat{\\R{Z}}|\\R{X}}(\\hat{\\R{Z}}|\\R{X})] + E[\\log p_{\\R{X}}(\\R{X})] - E[p_{\\hat{\\R{Z}}}(\\hat{\\R{Z}})] + E[- \\log p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|\\hat{\\R{Z}})] + \\gamma E[\\log p_{\\hat{\\R{Z}}}(\\hat{\\R{Z}})]- \\gamma E [\\log p_{\\R{Z}}(\\hat{\\R{Z}})]  - \\alpha E[\\log p_{\\hat{\\R{Z}}|\\R{X}}(\\hat{\\R{Z}}|\\R{X})] + \\alpha E[\\log_{\\hat{\\R{Z}}}(\\hat{\\R{Z}})]  \\\\\n",
    "&= E[- \\log p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|\\hat{\\R{Z}})] + (1-\\alpha)E[\\log p_{\\hat{\\R{Z}}|\\R{X}}(\\hat{\\R{Z}}|\\R{X})] +  (\\alpha + \\gamma -1)E[p_{\\hat{\\R{Z}}}(\\hat{\\R{Z}})] - \\gamma E [\\log p_{\\R{Z}}(\\hat{\\R{Z}})] + E[\\log p_{\\R{X}}(\\R{X})] \\\\\n",
    "&= E[- \\log p_{\\hat{\\R{X}}|\\R{Z}}(\\R{X}|\\hat{\\R{Z}})] +  (1-\\alpha)D(P_{\\hat{\\R{Z}}|\\R{X}}||P_{\\R{Z}}| P_{\\R{X}}) + (\\alpha + \\gamma -1)D(P_{\\hat{\\R{Z}}}||P_{\\R{Z}}) - \\underbrace{H(\\R{X})}_{\\text{constant}}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refer to the implementation by https://github.com/pratikm141/MMD-Variational-Autoencoder-Pytorch-InfoVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72988f6554239d4d9a59030431e3f9673f71a93641bf30a7410994f37c695aee"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
