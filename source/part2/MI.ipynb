{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb70a32d-8dc5-4b99-bcdf-15f6adb58d15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutual Information Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c1b3c-d6cd-47d8-9d06-778147e8650a",
   "metadata": {},
   "source": [
    "$\\def\\abs#1{\\left\\lvert #1 \\right\\rvert}\n",
    "\\def\\Set#1{\\left\\{ #1 \\right\\}}\n",
    "\\def\\mc#1{\\mathcal{#1}}\n",
    "\\def\\M#1{\\boldsymbol{#1}}\n",
    "\\def\\R#1{\\mathsf{#1}}\n",
    "\\def\\RM#1{\\boldsymbol{\\mathsf{#1}}}\n",
    "\\def\\op#1{\\operatorname{#1}}\n",
    "\\def\\E{\\op{E}}\n",
    "\\def\\d{\\mathrm{\\mathstrut d}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9909a95a-8dcf-42e8-a826-3f7819c4ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dv import *\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import tensorboard as tb\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5122355-6814-4dc9-aee0-41cc985dd99a",
   "metadata": {},
   "source": [
    "**How to estimate MI via KL divergence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff0ced-ebed-4598-ae4e-2d67ea8feaca",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce a few methods of estimating the mutual information via KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ae125-e9ab-4c63-a72b-a5003f916656",
   "metadata": {},
   "source": [
    "We first introduce the Mutual Information Neural Estimation (MINE) method in {cite}`belghazi2018mine`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54672af4-c076-476b-ac43-070c4af7b5e1",
   "metadata": {},
   "source": [
    "## MINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a638b-8984-4036-9d15-621f9dbf89cf",
   "metadata": {},
   "source": [
    "The idea is to obtain MI {eq}`MI` from KL divergence {eq}`D` as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be906a06-a763-410b-9098-ac57f9d918ca",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "I(\\R{X}\\wedge \\R{Y}) = D(\\underbrace{P_{\\R{X},\\R{Y}}}_{P_{\\R{Z}}}\\| \\underbrace{P_{\\R{X}}\\times P_{\\R{Y}}}_{P_{\\R{Z}'}}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7e9cf-28b2-4919-9c5c-2c8e7a68056e",
   "metadata": {},
   "source": [
    "and then apply the DV formula {eq}`avg-DV` to estimate the divergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c1971-d508-42d4-939e-9ec3a9981ce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Definition** MINE  \n",
    ":label: MINE\n",
    "\n",
    "The mutual information neural estimation (MINE) of $I(\\R{X}\\wedge\\R{Y})$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\R{I}_{\\text{MINE}} := \\sup_{t_{\\theta}: \\mc{Z} \\to \\mathbb{R}} \\overbrace{\\frac1n \\sum_{i\\in [n]} t_{\\theta}(\\R{X}_i,\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\theta}(\\R{X}'_i,\\R{Y}'_i)}}^{-\\R{L}_{\\text{MINE}}(\\theta):=}\n",
    "\\end{align}\n",
    "$$ (MINE)\n",
    "\n",
    "where \n",
    "\n",
    "- the supremum is over $t_{\\theta}$ representable by a neural network with trainable/optimizable parameters $\\theta$,\n",
    "- $P_{\\R{X}',\\R{Y}'}:=P_{\\R{X}}\\times P_{\\R{Y}}$, and\n",
    "- $(\\R{X}'_i,\\R{Y}'_i\\mid i\\in [n'])$ is the sequence of i.i.d. samples of $P_{\\R{X}',\\R{Y}'}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00017f18-d719-41f2-aa8e-53ff24f5e8e6",
   "metadata": {},
   "source": [
    "The above actually does not completely define MINE. There are some implementation details to be filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeba77f-0a85-4066-9696-90a73634d56f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**How to obtain the reference samples ${\\R{Z}'}^{n'}$, i.e., ${\\R{X}'}^{n'}$ and ${\\R{Y}'}^{n'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61052a26-0343-4e24-b18a-3899ac0617dc",
   "metadata": {},
   "source": [
    "We can approximate the i.i.d. sampling of $P_{\\R{X}}\\times P_{\\R{Y}}$ using samples from $P_{\\R{X},\\R{Y}}$ by a re-sampling trick:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3b0c0-7b33-4a32-a426-21a14a323a69",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P_{\\R{Z}'^{n'}} &\\approx P_{((\\R{X}_{\\R{J}_i},\\R{Y}_{\\R{K}_i})\\mid i \\in [n'])}\n",
    "\\end{align}\n",
    "$$ (resample)\n",
    "\n",
    "where $\\R{J}_i$ and $\\R{K}_i$ for $i\\in [n']$ are independent and uniformly random indices\n",
    "\n",
    "$$\n",
    "P_{\\R{J},\\R{K}} = \\op{Uniform}_{[n]\\times [n]}\n",
    "$$\n",
    "\n",
    "and $[n]:=\\Set{1,\\dots,n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c465a-2b21-42ec-843a-6715c2ba6339",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "MINE {cite}`belghazi2018mine` uses the following implementation that samples $(\\R{J},\\R{K})$ but without replacement. You can change $n'$ using the slider for `n_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8b5202-d666-4e3d-a861-0bd2358fb26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6979ef2b1fbd4373a85fb95358706dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=501, description='n_', max=1000, min=2), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "\n",
    "def resample(data, size, replace=False):\n",
    "    index = rng.choice(range(data.shape[0]), size=size, replace=replace)\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "@widgets.interact\n",
    "def plot_resampled_data_without_replacement(n_=(2, n)):\n",
    "    XY_ = np.block([resample(XY[:, [0]], n_), resample(XY[:, [1]], n_)])\n",
    "    resampled_data = pd.DataFrame(XY_, columns=[\"X'\", \"Y'\"])\n",
    "    p_ = plot_samples_with_kde(resampled_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6eba5e-d6e9-48fc-956d-694c470b6e5e",
   "metadata": {},
   "source": [
    "In the above, we defined the function `resample` that \n",
    "- uses `choice` to uniformly randomly select a choice\n",
    "- from a `range` of integers going from `0` to \n",
    "- the size of the first dimension of the `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75febee6-6e07-4b1b-8636-d88d241fdb56",
   "metadata": {},
   "source": [
    "Note however that the sampling is without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22285f9a-13a4-4fc0-8d48-b3ab8005f088",
   "metadata": {},
   "source": [
    "**Is it a good idea to sample without replacement?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710fac34-6635-4f55-b9b1-47e91fa017d1",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Sampling without replacement has an important restriction $n'\\leq n$. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247f302-7e15-4182-b6a9-7b8868cabf12",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "without-replacement",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Solution**\n",
    "\n",
    "To allow $n>n'$, at least one sample $(\\R{X}_i,\\R{Y}_i)$ needs to be sampled more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015b30c-0fd3-4e37-a3bd-7bdf389ed9e1",
   "metadata": {},
   "source": [
    "**Exercise** To allow $n>n'$, complete the following code to sample with replacement and observe what happens when $n \\gg n'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92dbcc28-c074-431b-8def-77d1b573b5d6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "resample",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55687214148848dda27d48cda9cba9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, continuous_update=False, description='n_', max=10000, min=2), Outp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact\n",
    "def plot_resampled_data_with_replacement(\n",
    "    n_=widgets.IntSlider(n, 2, 10 * n, continuous_update=False)\n",
    "):\n",
    "    ### BEGIN SOLUTION\n",
    "    XY_ = np.block(\n",
    "        [resample(XY[:, [0]], n_, replace=True), resample(XY[:, [1]], n_, replace=True)]\n",
    "    )\n",
    "    ### END SOLUTION\n",
    "    resampled_data = pd.DataFrame(XY_, columns=[\"X'\", \"Y'\"])\n",
    "    p_ = plot_samples_with_kde(resampled_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcd98e-d07b-426b-9724-52a16e76edf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Exercise** \n",
    "\n",
    "Explain whether the resampling trick gives i.i.d. samples $(\\R{X}_{\\R{J}_i},\\R{Y}_{\\R{K}_i})$ for the cases with replacement and without replacement respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b046f-629e-4151-ba09-7dac76de8b7d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "non-iid",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "**Solution** \n",
    "\n",
    "The samples are identically distributed. However, they are not independent except in the trivial case $n=1$ or $n'=1$, regardless of whether the sample is with replacement or not. Consider $n=1$ and $n'=2$ as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963bca6-3230-484e-bca6-15dac88f6352",
   "metadata": {},
   "source": [
    "To improve the stability of the training, MINE applies additional smoothing to the gradient calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa76d5-a4cb-4d80-a37f-ef527d537112",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\R{L}_{\\text{MINE}}(\\theta) &= \\overbrace{- \\frac{1}{n} \\sum_{i\\in [n]}  t_{\\theta} (\\R{X}_i, \\R{Y}_i) }^{\\R{L}_1(\\theta):=} + \\log \\overbrace{\\frac{1}{n'} \\sum_{i\\in [n']}  e^{t_{\\theta} (\\R{X}'_i, \\R{Y}'_i)}}^{\\R{L}_2(\\theta):=}\\\\\n",
    "\\nabla \\R{L}_{\\text{MINE}}(\\theta) &= \\nabla \\R{L}_1(\\theta) + \\frac{\\nabla \\R{L}_2(\\theta)}{\\R{L}_2(\\theta)}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c8129-33d1-4127-a40c-0592cecb1368",
   "metadata": {},
   "source": [
    "Variation in $\\nabla \\R{L}_2(\\theta)$ leads to the variation of the overall gradient especially when $\\R{L}_2(\\theta)$ is small. With minibatch gradient descent, the sample average is over a small batch and so the variance can be quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6c0a3-b9de-47ff-b2ab-27d7b56fe40b",
   "metadata": {},
   "source": [
    "To alleviate such variation, MINE replaces the denominator $\\R{L}_2(\\theta)$ by its moving average:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b409c4-4eab-457b-abf3-1eace29740a0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta^{(i+1)} := \\theta^{(i)} - s^{(i)} \\nabla \\R{L}_1 (\\theta^{(i)}) + \\frac{\\nabla \\R{L}_2(\\theta^{(i)})}{\\overline{\\R{L}}_2^{(i)}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\overline{\\R{L}}_2^{(i)} =  \\beta \\overline{\\R{L}}_2^{(i-1)} + (1-\\beta) \\R{L}_2(\\theta^{(i)})\n",
    "$$\n",
    "\n",
    "for some smoothing factor $\\beta\\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9135e-85d5-4bf8-b33f-00802377d76c",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Implement a neural network trainer for MINE similar to `DVTrainer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e9f8ae-93e6-479b-be48-081e66ab6413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDVTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mDVTrainer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# set optimizer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ_ref\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# batch sizes\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iters_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m  \u001b[0;31m# ideally a divisor of both n and n'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# logging\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mwriter_params\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m  \u001b[0;31m# create a new folder under runs/ for logging\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# keep counts for logging\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# random indices for selecting samples for all batches in one epoch\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0midx_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;31m# obtain a random batch of samples\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbatch_Z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbatch_Z_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0midx_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_ref\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;31m# define the loss as negative DV divergence lower bound\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mDV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_Z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Z_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# calculate gradient\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# descend\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mestimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Estimate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miml/source/part2/dv.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DVTrainer??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56f6dd-e301-4c0e-acd2-ed9c17f79d20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MI-NEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522dede-66d5-44ad-b508-3ac675793d4c",
   "metadata": {},
   "source": [
    "**Is it possible to generate i.i.d. samples for ${\\R{Z}'}^{n'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584a25f-755d-4e98-8307-5cf3c37b7654",
   "metadata": {},
   "source": [
    "Consider another formula for mutual information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc81b16-5238-477c-80b2-48b2c528f4bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Proposition**  \n",
    ":label: MI-3D\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}) - D(P_{\\R{X}}\\|P_{\\R{X}'}) - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\n",
    "\\end{align}\n",
    "$$ (MI-3D)\n",
    "\n",
    "for any product reference distribution $P_{\\R{X}'}\\times P_{\\R{Y}'}$ for which the divergences are finite.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc1af9-d7e8-49f3-9509-ed77a7429f75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Corollary**  \n",
    ":label: MI-ub\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= \\inf_{\\substack{P_{\\R{X}'}\\in \\mc{P}(\\mc{X})\\\\ P_{\\R{Y}'}\\in \\mc{P}(\\mc{Y})}} D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}).\n",
    "\\end{align}\n",
    "$$ (MI-ub)\n",
    "\n",
    "where the optimal solution is $P_{\\R{X}'}\\times P_{\\R{Y}'}=P_{\\R{X}}\\times P_{\\R{Y}}$, the product of marginal distributions of $\\R{X}$ and $\\R{Y}$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf194fa-fa9a-4d17-a70f-213c07e4b1f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Proof**\n",
    "\n",
    "{eq}`MI-ub` follows from {eq}`MI-3D` directly since the divergences are non-negative. To prove the proposition:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= H(\\R{X}) + H(\\R{Y}) - H(\\R{X},\\R{Y})\\\\\n",
    "&= E\\left[-\\log dP_{\\R{X}'}(\\R{X})\\right] - D(P_{\\R{X}}\\|P_{\\R{X}'})\\\\\n",
    "&\\quad+E\\left[-\\log dP_{\\R{Y}'}(\\R{Y})\\right] - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\\\\\n",
    "&\\quad-E\\left[-\\log d(P_{\\R{X}'}\\times P_{\\R{Y}'})(\\R{X},\\R{Y})\\right] + D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'})\\\\\n",
    "&= D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}) - D(P_{\\R{X}}\\|P_{\\R{X}'}) - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fecbc-9c51-47c6-b88b-7e1a5adeee5f",
   "metadata": {},
   "source": [
    "*Mutual Information Neural Entropic Estimation (MI-NEE)* {cite}`chan2019neural` uses {eq}`MI-3D` to estimate MI by estimating the three divergences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c956612-7bea-4ddc-a0fc-e36edd3fc439",
   "metadata": {},
   "source": [
    "Applying {eq}`avg-DV` to each divergence in {eq}`MI-3D`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fa4fc-f584-4bda-b570-2e0d5aa1b590",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &\\approx \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{X},\\R{Y}}(\\R{X}_i,\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{X},\\R{Y}}(\\R{X}'_i,\\R{Y}'_i)}\\\\\n",
    "&\\quad - \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{X}}(\\R{X}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{X}}(\\R{X}'_i)} \\\\\n",
    "&\\quad - \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{Y}}(\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{Y}}(\\R{Y}'_i)}\n",
    "\\end{align}\n",
    "$$ (MI-NEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7eb69-7474-4e14-9cfb-d4faea394d7c",
   "metadata": {},
   "source": [
    "$P_{\\R{X}'}$ and $P_{\\R{Y}'}$ are known distributions and so arbitrarily many i.i.d. samples can be drawn from them directly without using the resampling trick {eq}`resample`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4576dba-9848-4689-b84e-59087d83ccd4",
   "metadata": {},
   "source": [
    "Indeed, since the choice of $P_{\\R{X}'}$ and $P_{\\R{Y}'}$ are arbitrary, we can also also train neural networks to optimize them. In particular, {eq}`MI-ub` is a special case where we can train neural networks to approximate $P_{\\R{X}}$ and $P_{\\R{Y}}$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
