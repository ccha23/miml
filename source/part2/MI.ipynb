{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb70a32d-8dc5-4b99-bcdf-15f6adb58d15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutual Information Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c1b3c-d6cd-47d8-9d06-778147e8650a",
   "metadata": {},
   "source": [
    "$\\def\\abs#1{\\left\\lvert #1 \\right\\rvert}\n",
    "\\def\\Set#1{\\left\\{ #1 \\right\\}}\n",
    "\\def\\mc#1{\\mathcal{#1}}\n",
    "\\def\\M#1{\\boldsymbol{#1}}\n",
    "\\def\\R#1{\\mathsf{#1}}\n",
    "\\def\\RM#1{\\boldsymbol{\\mathsf{#1}}}\n",
    "\\def\\op#1{\\operatorname{#1}}\n",
    "\\def\\E{\\op{E}}\n",
    "\\def\\d{\\mathrm{\\mathstrut d}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9909a95a-8dcf-42e8-a826-3f7819c4ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from dv import *\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import tensorboard as tb\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5122355-6814-4dc9-aee0-41cc985dd99a",
   "metadata": {},
   "source": [
    "**How to estimate MI via KL divergence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff0ced-ebed-4598-ae4e-2d67ea8feaca",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce a few methods of estimating the mutual information (Definition {ref}`MI-estimation`) via KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ae125-e9ab-4c63-a72b-a5003f916656",
   "metadata": {},
   "source": [
    "We first introduce the Mutual Information Neural Estimation (MINE) method in {cite}`belghazi2018mine`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54672af4-c076-476b-ac43-070c4af7b5e1",
   "metadata": {},
   "source": [
    "## MINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a638b-8984-4036-9d15-621f9dbf89cf",
   "metadata": {},
   "source": [
    "The idea is to obtain MI {eq}`MI` from KL divergence {eq}`D` as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be906a06-a763-410b-9098-ac57f9d918ca",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "I(\\R{X}\\wedge \\R{Y}) = D(\\underbrace{P_{\\R{X},\\R{Y}}}_{P_{\\R{Z}}}\\| \\underbrace{P_{\\R{X}}\\times P_{\\R{Y}}}_{P_{\\R{Z}'}}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7e9cf-28b2-4919-9c5c-2c8e7a68056e",
   "metadata": {},
   "source": [
    "and then apply the DV formula {eq}`avg-DV` to estimate the divergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c1971-d508-42d4-939e-9ec3a9981ce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Definition** MINE  \n",
    ":label: MINE\n",
    "\n",
    "The idea of mutual information neural estimation (MINE) is to estimate $I(\\R{X}\\wedge\\R{Y})$ by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sup_{t_{\\theta}: \\mc{Z} \\to \\mathbb{R}} \\overbrace{\\frac1n \\sum_{i\\in [n]} t_{\\theta}(\\R{X}_i,\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\theta}(\\R{X}'_i,\\R{Y}'_i)}}^{\\R{I}_{\\text{MINE}}(\\theta):=}\n",
    "\\end{align}\n",
    "$$ (MINE)\n",
    "\n",
    "where \n",
    "\n",
    "- the supremum is over $t_{\\theta}$ representable by a neural network with trainable/optimizable parameters $\\theta$,\n",
    "- $P_{\\R{X}',\\R{Y}'}:=P_{\\R{X}}\\times P_{\\R{Y}}$, and\n",
    "- $(\\R{X}'_i,\\R{Y}'_i\\mid i\\in [n'])$ is the sequence of i.i.d. samples of $P_{\\R{X}',\\R{Y}'}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00017f18-d719-41f2-aa8e-53ff24f5e8e6",
   "metadata": {},
   "source": [
    "The above is not a complete description of MINE because there are some implementation details yet to be filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4ee28-8d6c-42d9-a6e7-9ae0efb4e358",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeba77f-0a85-4066-9696-90a73634d56f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**How to obtain the reference samples ${\\R{Z}'}^{n'}$, i.e., ${\\R{X}'}^{n'}$ and ${\\R{Y}'}^{n'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61052a26-0343-4e24-b18a-3899ac0617dc",
   "metadata": {},
   "source": [
    "We can approximate the i.i.d. sampling of $P_{\\R{X}}\\times P_{\\R{Y}}$ using samples from $P_{\\R{X},\\R{Y}}$ by a re-sampling trick:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3b0c0-7b33-4a32-a426-21a14a323a69",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P_{\\R{Z}'^{n'}} &\\approx P_{((\\R{X}_{\\R{J}_i},\\R{Y}_{\\R{K}_i})\\mid i \\in [n'])}\n",
    "\\end{align}\n",
    "$$ (resample)\n",
    "\n",
    "where $\\R{J}_i$ and $\\R{K}_i$ for $i\\in [n']$ are independent and uniformly random indices\n",
    "\n",
    "$$\n",
    "P_{\\R{J},\\R{K}} = \\op{Uniform}_{[n]\\times [n]}\n",
    "$$\n",
    "\n",
    "and $[n]:=\\Set{1,\\dots,n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c465a-2b21-42ec-843a-6715c2ba6339",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "MINE {cite}`belghazi2018mine` uses the following implementation that samples $(\\R{J},\\R{K})$ but without replacement. You can change $n'$ using the slider for `n_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8b5202-d666-4e3d-a861-0bd2358fb26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490b36c15fc64492a64b8fa4cadd3424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, continuous_update=False, description='n_', max=1000, min=2), Outpu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "\n",
    "def resample(data, size, replace=False):\n",
    "    index = rng.choice(range(data.shape[0]), size=size, replace=replace)\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "@widgets.interact(n_=widgets.IntSlider(n, 2, n, continuous_update=False))\n",
    "def plot_resampled_data_without_replacement(n_):\n",
    "    XY_ = np.block([resample(XY[:, [0]], n_), resample(XY[:, [1]], n_)])\n",
    "    resampled_data = pd.DataFrame(XY_, columns=[\"X'\", \"Y'\"])\n",
    "    p_ = plot_samples_with_kde(resampled_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6eba5e-d6e9-48fc-956d-694c470b6e5e",
   "metadata": {},
   "source": [
    "In the above, the function `resample`\n",
    "- uses `choice` to uniformly randomly select a choice\n",
    "- from a `range` of integers going from `0` to \n",
    "- the size of the first dimension of the `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75febee6-6e07-4b1b-8636-d88d241fdb56",
   "metadata": {},
   "source": [
    "Note however that the sampling is without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22285f9a-13a4-4fc0-8d48-b3ab8005f088",
   "metadata": {},
   "source": [
    "**Is it a good idea to sample without replacement?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710fac34-6635-4f55-b9b1-47e91fa017d1",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Sampling without replacement has an important restriction $n'\\leq n$. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247f302-7e15-4182-b6a9-7b8868cabf12",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "without-replacement",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Solution**\n",
    "\n",
    "To allow $n>n'$, at least one sample $(\\R{X}_i,\\R{Y}_i)$ needs to be sampled more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015b30c-0fd3-4e37-a3bd-7bdf389ed9e1",
   "metadata": {},
   "source": [
    "**Exercise** To allow $n>n'$, complete the following code to sample with replacement and observe what happens when $n \\gg n'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92dbcc28-c074-431b-8def-77d1b573b5d6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "resample",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16db2aa74a54c3f8dd9f460d649bc54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, continuous_update=False, description='n_', max=10000, min=2), Outp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(n_=widgets.IntSlider(n, 2, 10 * n, continuous_update=False))\n",
    "def plot_resampled_data_with_replacement(n_):\n",
    "    ### BEGIN SOLUTION\n",
    "    XY_ = np.block(\n",
    "        [resample(XY[:, [0]], n_, replace=True), resample(XY[:, [1]], n_, replace=True)]\n",
    "    )\n",
    "    ### END SOLUTION\n",
    "    resampled_data = pd.DataFrame(XY_, columns=[\"X'\", \"Y'\"])\n",
    "    p_ = plot_samples_with_kde(resampled_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcd98e-d07b-426b-9724-52a16e76edf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Exercise** \n",
    "\n",
    "Explain whether the resampling trick gives i.i.d. samples $(\\R{X}_{\\R{J}_i},\\R{Y}_{\\R{K}_i})$ for the cases with replacement and without replacement respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b046f-629e-4151-ba09-7dac76de8b7d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "non-iid",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "**Solution** \n",
    "\n",
    "The samples are identically distributed. However, they are not independent except in the trivial case $n=1$ or $n'=1$, regardless of whether the sample is with replacement or not. Consider $n=1$ and $n'=2$ as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b85aef-f2d2-48a9-a1b8-86276771d84b",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963bca6-3230-484e-bca6-15dac88f6352",
   "metadata": {},
   "source": [
    "To improve the stability of the training, MINE applies additional smoothing to the gradient calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915b1f1-5e56-4520-a783-de10ab3da725",
   "metadata": {},
   "source": [
    "**How to train the neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc5c71-0973-4e13-9433-639c32a2fabd",
   "metadata": {},
   "source": [
    "{eq}`MINE` can be solved iteratively by minibatch gradient descent using the loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\R{L}_{\\text{MINE}}(\\theta) &:= \\overbrace{- \\frac{1}{\\abs{\\R{B}}} \\sum_{i\\in \\R{B}}  t_{\\theta} (\\R{X}_i, \\R{Y}_i) }^{\\R{L}(\\theta):=} + \\log \\overbrace{\\frac{1}{\\abs{\\R{B}'}} \\sum_{i\\in \\R{B}'}  e^{t_{\\theta} (\\R{X}'_i, \\R{Y}'_i)}}^{\\R{L}'(\\theta):=}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\R{B}$ and $\\R{B}'$ are batches of uniformly randomly chosen indices from $[n]$ and $[n']$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42f67f-4558-450d-9182-3ebca19af445",
   "metadata": {},
   "source": [
    "The gradient expressed in terms of $\\R{L}$ and $\\R{L}'$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\R{L}_{\\text{MINE}}(\\theta) &= \\nabla \\R{L}(\\theta) + \\frac{\\nabla \\R{L}'(\\theta)}{\\R{L}'(\\theta)}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c8129-33d1-4127-a40c-0592cecb1368",
   "metadata": {},
   "source": [
    "Variation in $\\nabla \\R{L}'(\\theta)$ leads to the variation of the overall gradient especially when $\\R{L}'(\\theta)$ is small. With minibatch gradient descent, the sample average is over a small batch and so the variance can be quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b215572-5cb5-43f8-a3f6-df75965f7407",
   "metadata": {},
   "source": [
    "**How to reduce the variation in the gradient approximation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6c0a3-b9de-47ff-b2ab-27d7b56fe40b",
   "metadata": {},
   "source": [
    "To alleviate the variation, MINE replaces the denominator $\\R{L}'(\\theta)$ by its moving average:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b409c4-4eab-457b-abf3-1eace29740a0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{j+1} := \\theta_j - s_j \\underbrace{\\left(\\nabla \\R{L}_j (\\theta_j) + \\frac{\\nabla \\R{L}'_j(\\theta_j)}{\\overline{\\R{L}'}_j}\\right)}_{\\text{(*)}}\n",
    "$$ (MINE:update)\n",
    "\n",
    "where $\\R{L}_j$ and $\\R{L}'_j$ are the \n",
    "\n",
    "$$\n",
    "\\overline{\\R{L}'}_j =  \\beta \\overline{\\R{L}'}_{j-1} + (1-\\beta) \\R{L}'(\\theta_j)\n",
    "$$ (MINE:L2bar)\n",
    "\n",
    "for some smoothing factor $\\beta\\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828190f9-9895-4397-b694-a9fce60f04ee",
   "metadata": {},
   "source": [
    "**How to implement the smoothing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3e59d-034d-4e3b-8e10-1109b3671716",
   "metadata": {},
   "source": [
    "Instead of implementing a new optimizer, a simpler way is to redefine the loss at each step $i$ as follows {cite}`belghazi2018mine`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8dc917-20ad-41cb-9399-ebf7693baa06",
   "metadata": {},
   "source": [
    "$$\n",
    "\\R{L}_{\\text{MINE},j}(\\theta) = \\R{L}_j(\\theta) + \\frac{\\R{L}'_j(\\theta)}{\\overline{\\R{L}'}_j}\n",
    "$$ (MINE:mv)\n",
    "\n",
    "where $\\overline{\\R{L}'}_j$ in {eq}`MINE:L2bar` is regarded as a constant in calculating the gradient. It is easy to verify that $\\nabla \\R{L}_{\\text{MINE},j}(\\theta_j)$ gives the desired gradient (*) in {eq}`MINE:update`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4b866-fcf9-4799-a513-2537dd0021d7",
   "metadata": {},
   "source": [
    "In summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b6b72-6fca-4084-a7e2-54051b766e46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "MINE estimates the mutual information $I(\\R{X}\\wedge\\R{Y})$ as $\\R{I}_{\\text{MINE}}(\\theta_j)$ {eq}`MINE` where $\\theta_j$ is updated by descending along the gradient of $\\R{L}_{\\text{MINE},j}$ {eq}`MINE:mv` iteratively after $j$ steps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b06889d-13b1-4042-98e8-11ac0c835d4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad621d-14ff-4b8d-a2a6-62485eb817a0",
   "metadata": {},
   "source": [
    "Consider implementing MINE for the jointly gaussian $\\R{X}$ and $\\R{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44e0261d-7d11-45e0-abb0-f6c04027db92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 1]), torch.Size([1000, 1]), 0.7405246745135301)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 0\n",
    "\n",
    "# set device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# create samples\n",
    "XY_rng = np.random.default_rng(SEED)\n",
    "rho = 1 - 0.19 * XY_rng.random()\n",
    "mean, cov, n = [0, 0], [[1, rho], [rho, 1]], 1000\n",
    "XY = XY_rng.multivariate_normal(mean, cov, n)\n",
    "\n",
    "X = Tensor(XY[:, [0]]).to(DEVICE)\n",
    "Y = Tensor(XY[:, [1]]).to(DEVICE)\n",
    "ground_truth = -0.5 * np.log(1 - rho ** 2)\n",
    "X.shape, Y.shape, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc440b3-6c21-4375-a974-6200d9d2dfe1",
   "metadata": {},
   "source": [
    "The tensors `X` and `Y` correspond to samples of $\\R{X}$ and $\\R{Y}$ respectively. The first dimension indices the different samples. The `ground_truth` is the actual mutual information $I(\\R{X}\\wedge\\R{Y})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3080cf-970f-4709-8736-85f7f632d0aa",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Complete the definition of `forward` so that the neural network $t_{\\theta}$ is a vectorized function that takes samples `x` and `y` of $\\R{X}$ and $\\R{Y}$ as input and approximates the density ratio $\\frac{P_{\\R{X},\\R{Y}}}{P_{\\R{X}}\\times P_{\\R{Y}}}$ at `(x, y)`:\n",
    "\n",
    "![Neural network](nn.dio.svg)\n",
    "\n",
    "- Use `torch.cat` to concatenate tensors `x` and `y` in the last dimension.\n",
    "- Use `F.elu` for the activation function $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8e21eb10-4f63-47f9-83dd-1be4b1821d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=100, sigma=0.02):\n",
    "        super().__init__()\n",
    "        # fully-connected (fc) layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  # layer 2\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)  # layer 3\n",
    "        nn.init.normal_(self.fc1.weight, std=sigma)  #\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.normal_(self.fc2.weight, std=sigma)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        nn.init.normal_(self.fc3.weight, std=sigma)\n",
    "        nn.init.constant_(self.fc3.bias, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "        Vectorized function that implements the neural network t(x,y).\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x, y: 2D Tensors where the first dimensions index different samples.\n",
    "        \"\"\"\n",
    "        ### BEGIN SOLUTION\n",
    "        a1 = F.elu(self.fc1(torch.cat([x, y], dim=-1)))\n",
    "        a2 = F.elu(self.fc2(a1))\n",
    "        t = self.fc3(a2)\n",
    "        ### END SOLUTION\n",
    "        return t\n",
    "\n",
    "    def plot(self, xmin=-5, xmax=5, ymin=-5, ymax=5, xgrids=50, ygrids=50, ax=None):\n",
    "        \"\"\"Plot a heat map of a neural network net. net can only have two inputs.\"\"\"\n",
    "        x, y = np.mgrid[xmin : xmax : xgrids * 1j, ymin : ymax : ygrids * 1j]\n",
    "        with torch.no_grad():\n",
    "            z = net(Tensor(x.reshape(-1,1)).to(DEVICE), Tensor(y.reshape(-1,1)).to(DEVICE)).reshape(x.shape).cpu()\n",
    "        if ax is None:\n",
    "            ax = plt.gca()\n",
    "        im = ax.pcolormesh(x, y, z, cmap=\"RdBu_r\", shading=\"auto\")\n",
    "        ax.figure.colorbar(im)\n",
    "        ax.set(xlabel=r\"$x$\", ylabel=r\"$y$\", title=r\"Heatmap of $t(x,y)$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fee3f7-88f8-4f7f-ae59-aa8f2f0bf887",
   "metadata": {},
   "source": [
    "To create and plot the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ff1dd7c7-633b-4ece-9d38-27b71472155a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f420727c094ee081a4b88fd5e63df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "net = Net().to(DEVICE)\n",
    "plt.figure()\n",
    "net.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c489a0-b1b1-41cc-9b51-d1d9fac8bb1d",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "To implement a neural network trainer for MINE similar to `DVTrainer`, completes the definition of `loss` according to {eq}`MINE:mv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1c470412-c255-4008-b8e4-6b42ee3e0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MINETrainer:\n",
    "    def __init__(self, X, Y, net, n_iters_per_epoch, writer_params={}, **kwargs):\n",
    "        \"\"\"\n",
    "        Neural estimator for Mutual Information based on MINE.\n",
    "\n",
    "        Estimate I(X;Y) using samples of X and Y by training a network t to maximize\n",
    "            avg(t(X,Y)) - avg(e^t(X',Y')) / m\n",
    "        where samples of (X',Y') approximates P_X * P_Y using the resampling trick, and\n",
    "        m is obtained by smoothing avg(e^t(X',Y')) with the factor beta.\n",
    "\n",
    "        parameters:\n",
    "        -----------\n",
    "\n",
    "        X, Y : Tensors with first dimensions of the same size indicing the samples.\n",
    "        net  : The neural network t that takes X and Y as input and output a real number for each a real number for each sample.\n",
    "        n_iters_per_epoch : Number of iterations per epoch.\n",
    "        beta : Smoothing/forgetting factor between [0,1]\n",
    "        writer_params     : Parameters to be passed to SummaryWriter for logging.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.n = min(X.shape[0], Y.shape[0])\n",
    "        self.beta = 0.9\n",
    "        self.m = 1\n",
    "        self.net = net\n",
    "\n",
    "        # set optimizer\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), **kwargs)\n",
    "\n",
    "        # batch sizes\n",
    "        self.n_iters_per_epoch = n_iters_per_epoch\n",
    "        self.batch_size = int((self.n + 0.5) / n_iters_per_epoch)\n",
    "\n",
    "        # logging\n",
    "        self.writer = SummaryWriter(**writer_params)\n",
    "        self.n_iter = self.n_epoch = 0\n",
    "\n",
    "    def step(self, epochs=1):\n",
    "        \"\"\"\n",
    "        Carries out the gradient descend for a number of epochs and returns the MI estimate evaluated over the entire data.\n",
    "\n",
    "        Loss for each epoch is recorded into the log, but only one MI estimate is computed/logged using the entire dataset.\n",
    "        Rerun the method to continue to train the neural network and log the results.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        epochs : number of epochs\n",
    "        \"\"\"\n",
    "        for i in range(epochs):\n",
    "            self.n_epoch += 1\n",
    "\n",
    "            # random indices for selecting samples for all batches in one epoch\n",
    "            idx = torch.randperm(self.n)\n",
    "\n",
    "            # resampling to approximate the sampling of (X',Y')\n",
    "            idx_X = torch.randperm(self.n)\n",
    "            idx_Y = torch.randperm(self.n)\n",
    "\n",
    "            # obtain a random batch of samples\n",
    "            batch_X = self.X[idx[i : self.n : self.batch_size]]\n",
    "            batch_Y = self.Y[idx[i : self.n : self.batch_size]]\n",
    "            batch_X_ref = self.X[idx_X[i : self.n : self.batch_size]]\n",
    "            batch_Y_ref = self.Y[idx_Y[i : self.n : self.batch_size]]\n",
    "\n",
    "            # define the loss\n",
    "            ### BEGIN SOLUTION\n",
    "            L = -net(batch_X, batch_Y).mean()\n",
    "            L_ = net(batch_X_ref, batch_Y_ref).exp().mean()\n",
    "            self.m = (1 - self.beta) * L_.detach() + self.beta * self.m\n",
    "            loss = L + L_ / self.m\n",
    "            ### END SOLUTION\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.writer.add_scalar(\"Loss/train\", loss.item(), global_step=self.n_iter)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            idx_X = torch.randperm(self.n)\n",
    "            idx_Y = torch.randperm(self.n)\n",
    "            X_ref = self.X[idx_X]\n",
    "            Y_ref = self.Y[idx_Y]\n",
    "            estimate = (\n",
    "                net(self.X, self.Y).mean()\n",
    "                - net(X_ref, Y_ref).logsumexp(0)\n",
    "                - np.log(self.n)\n",
    "            )\n",
    "            self.writer.add_scalar(\"Estimate\", estimate, global_step=self.n_epoch)\n",
    "            return estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65580f6-82c8-4fff-885c-5d59f5cda58d",
   "metadata": {},
   "source": [
    "To create the trainer for MINE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "48825421-d268-41fc-b3de-c74de4cf553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MINETrainer(X, Y, net, n_iters_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb38918-9212-4b91-ba9d-e6d927b594cd",
   "metadata": {},
   "source": [
    "To train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "45ada539-a0f8-4c8e-9421-41312644200e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Train? [Y/n] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d32d3f3ccdf43f383dd3e7676830a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5353/2707829606.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train? [Y/n]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"n\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             )\n\u001b[0;32m--> 981\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "if input(\"Train? [Y/n]\").lower() != \"n\":\n",
    "    for i in range(10):\n",
    "        trainer.step(10)\n",
    "    plt.figure()\n",
    "    net.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b11c6a0a-82de-45f1-824e-5dc641ab4cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7831), started 0:03:37 ago. (Use '!kill 7831' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d4690a13b29486de\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d4690a13b29486de\");\n",
       "          const url = new URL(\"/user/ccha23@gapps.cityu.edu.hk/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56f6dd-e301-4c0e-acd2-ed9c17f79d20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MI-NEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522dede-66d5-44ad-b508-3ac675793d4c",
   "metadata": {},
   "source": [
    "**Is it possible to generate i.i.d. samples for ${\\R{Z}'}^{n'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584a25f-755d-4e98-8307-5cf3c37b7654",
   "metadata": {},
   "source": [
    "Consider another formula for mutual information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc81b16-5238-477c-80b2-48b2c528f4bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Proposition**  \n",
    ":label: MI-3D\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}) - D(P_{\\R{X}}\\|P_{\\R{X}'}) - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\n",
    "\\end{align}\n",
    "$$ (MI-3D)\n",
    "\n",
    "for any product reference distribution $P_{\\R{X}'}\\times P_{\\R{Y}'}$ for which the divergences are finite.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc1af9-d7e8-49f3-9509-ed77a7429f75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Corollary**  \n",
    ":label: MI-ub\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= \\inf_{\\substack{P_{\\R{X}'}\\in \\mc{P}(\\mc{X})\\\\ P_{\\R{Y}'}\\in \\mc{P}(\\mc{Y})}} D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}).\n",
    "\\end{align}\n",
    "$$ (MI-ub)\n",
    "\n",
    "where the optimal solution is $P_{\\R{X}'}\\times P_{\\R{Y}'}=P_{\\R{X}}\\times P_{\\R{Y}}$, the product of marginal distributions of $\\R{X}$ and $\\R{Y}$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf194fa-fa9a-4d17-a70f-213c07e4b1f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Proof**\n",
    "\n",
    "{eq}`MI-ub` follows from {eq}`MI-3D` directly since the divergences are non-negative. To prove the proposition:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= H(\\R{X}) + H(\\R{Y}) - H(\\R{X},\\R{Y})\\\\\n",
    "&= E\\left[-\\log dP_{\\R{X}'}(\\R{X})\\right] - D(P_{\\R{X}}\\|P_{\\R{X}'})\\\\\n",
    "&\\quad+E\\left[-\\log dP_{\\R{Y}'}(\\R{Y})\\right] - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\\\\\n",
    "&\\quad-E\\left[-\\log d(P_{\\R{X}'}\\times P_{\\R{Y}'})(\\R{X},\\R{Y})\\right] + D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'})\\\\\n",
    "&= D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}) - D(P_{\\R{X}}\\|P_{\\R{X}'}) - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fecbc-9c51-47c6-b88b-7e1a5adeee5f",
   "metadata": {},
   "source": [
    "*Mutual Information Neural Entropic Estimation (MI-NEE)* {cite}`chan2019neural` uses {eq}`MI-3D` to estimate MI by estimating the three divergences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c956612-7bea-4ddc-a0fc-e36edd3fc439",
   "metadata": {},
   "source": [
    "Applying {eq}`avg-DV` to each divergence in {eq}`MI-3D`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fa4fc-f584-4bda-b570-2e0d5aa1b590",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &\\approx \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{X},\\R{Y}}(\\R{X}_i,\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{X},\\R{Y}}(\\R{X}'_i,\\R{Y}'_i)}\\\\\n",
    "&\\quad - \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{X}}(\\R{X}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{X}}(\\R{X}'_i)} \\\\\n",
    "&\\quad - \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{Y}}(\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{Y}}(\\R{Y}'_i)}\n",
    "\\end{align}\n",
    "$$ (MI-NEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7eb69-7474-4e14-9cfb-d4faea394d7c",
   "metadata": {},
   "source": [
    "$P_{\\R{X}'}$ and $P_{\\R{Y}'}$ are known distributions and so arbitrarily many i.i.d. samples can be drawn from them directly without using the resampling trick {eq}`resample`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4576dba-9848-4689-b84e-59087d83ccd4",
   "metadata": {},
   "source": [
    "Indeed, since the choice of $P_{\\R{X}'}$ and $P_{\\R{Y}'}$ are arbitrary, we can also also train neural networks to optimize them. In particular, {eq}`MI-ub` is a special case where we can train neural networks to approximate $P_{\\R{X}}$ and $P_{\\R{Y}}$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
