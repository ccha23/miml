{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb70a32d-8dc5-4b99-bcdf-15f6adb58d15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mutual Information Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c1b3c-d6cd-47d8-9d06-778147e8650a",
   "metadata": {},
   "source": [
    "$\\def\\abs#1{\\left\\lvert #1 \\right\\rvert}\n",
    "\\def\\Set#1{\\left\\{ #1 \\right\\}}\n",
    "\\def\\mc#1{\\mathcal{#1}}\n",
    "\\def\\M#1{\\boldsymbol{#1}}\n",
    "\\def\\R#1{\\mathsf{#1}}\n",
    "\\def\\RM#1{\\boldsymbol{\\mathsf{#1}}}\n",
    "\\def\\op#1{\\operatorname{#1}}\n",
    "\\def\\E{\\op{E}}\n",
    "\\def\\d{\\mathrm{\\mathstrut d}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9909a95a-8dcf-42e8-a826-3f7819c4ad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from dv import *\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "import tensorboard as tb\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5122355-6814-4dc9-aee0-41cc985dd99a",
   "metadata": {},
   "source": [
    "**How to estimate MI via KL divergence?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff0ced-ebed-4598-ae4e-2d67ea8feaca",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce a few methods of estimating the mutual information (Definition {ref}`MI-estimation`) via KL divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9ae125-e9ab-4c63-a72b-a5003f916656",
   "metadata": {},
   "source": [
    "We first introduce the Mutual Information Neural Estimation (MINE) method in {cite}`belghazi2018mine`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54672af4-c076-476b-ac43-070c4af7b5e1",
   "metadata": {},
   "source": [
    "## MINE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a638b-8984-4036-9d15-621f9dbf89cf",
   "metadata": {},
   "source": [
    "The idea is to obtain MI {eq}`MI` from KL divergence {eq}`D` as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be906a06-a763-410b-9098-ac57f9d918ca",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "I(\\R{X}\\wedge \\R{Y}) = D(\\underbrace{P_{\\R{X},\\R{Y}}}_{P_{\\R{Z}}}\\| \\underbrace{P_{\\R{X}}\\times P_{\\R{Y}}}_{P_{\\R{Z}'}}).\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7e9cf-28b2-4919-9c5c-2c8e7a68056e",
   "metadata": {},
   "source": [
    "and then apply the DV formula {eq}`avg-DV` to estimate the divergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c1971-d508-42d4-939e-9ec3a9981ce3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Definition** MINE  \n",
    ":label: MINE\n",
    "\n",
    "The mutual information neural estimation (MINE) of $I(\\R{X}\\wedge\\R{Y})$ is\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sup_{t_{\\theta}: \\mc{Z} \\to \\mathbb{R}} \\overbrace{\\frac1n \\sum_{i\\in [n]} t_{\\theta}(\\R{X}_i,\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\theta}(\\R{X}'_i,\\R{Y}'_i)}}^{\\R{I}_{\\text{MINE}}(\\theta):=}\n",
    "\\end{align}\n",
    "$$ (MINE)\n",
    "\n",
    "where \n",
    "\n",
    "- the supremum is over $t_{\\theta}$ representable by a neural network with trainable/optimizable parameters $\\theta$,\n",
    "- $P_{\\R{X}',\\R{Y}'}:=P_{\\R{X}}\\times P_{\\R{Y}}$, and\n",
    "- $(\\R{X}'_i,\\R{Y}'_i\\mid i\\in [n'])$ is the sequence of i.i.d. samples of $P_{\\R{X}',\\R{Y}'}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00017f18-d719-41f2-aa8e-53ff24f5e8e6",
   "metadata": {},
   "source": [
    "The above is an overly simplified description of MINE because there are some implementation details to be filled in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b4ee28-8d6c-42d9-a6e7-9ae0efb4e358",
   "metadata": {},
   "source": [
    "### Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeba77f-0a85-4066-9696-90a73634d56f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**How to obtain the reference samples ${\\R{Z}'}^{n'}$, i.e., ${\\R{X}'}^{n'}$ and ${\\R{Y}'}^{n'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61052a26-0343-4e24-b18a-3899ac0617dc",
   "metadata": {},
   "source": [
    "We can approximate the i.i.d. sampling of $P_{\\R{X}}\\times P_{\\R{Y}}$ using samples from $P_{\\R{X},\\R{Y}}$ by a re-sampling trick:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c3b0c0-7b33-4a32-a426-21a14a323a69",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "P_{\\R{Z}'^{n'}} &\\approx P_{((\\R{X}_{\\R{J}_i},\\R{Y}_{\\R{K}_i})\\mid i \\in [n'])}\n",
    "\\end{align}\n",
    "$$ (resample)\n",
    "\n",
    "where $\\R{J}_i$ and $\\R{K}_i$ for $i\\in [n']$ are independent and uniformly random indices\n",
    "\n",
    "$$\n",
    "P_{\\R{J},\\R{K}} = \\op{Uniform}_{[n]\\times [n]}\n",
    "$$\n",
    "\n",
    "and $[n]:=\\Set{1,\\dots,n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c465a-2b21-42ec-843a-6715c2ba6339",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "MINE {cite}`belghazi2018mine` uses the following implementation that samples $(\\R{J},\\R{K})$ but without replacement. You can change $n'$ using the slider for `n_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d8b5202-d666-4e3d-a861-0bd2358fb26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7e6567299c4031ac5161799712e673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, continuous_update=False, description='n_', max=1000, min=2), Outpu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "\n",
    "def resample(data, size, replace=False):\n",
    "    index = rng.choice(range(data.shape[0]), size=size, replace=replace)\n",
    "    return data[index]\n",
    "\n",
    "\n",
    "@widgets.interact(n_=widgets.IntSlider(n, 2, n, continuous_update=False))\n",
    "def plot_resampled_data_without_replacement(n_):\n",
    "    XY_ = np.block([resample(XY[:, [0]], n_), resample(XY[:, [1]], n_)])\n",
    "    resampled_data = pd.DataFrame(XY_, columns=[\"X'\", \"Y'\"])\n",
    "    p_ = plot_samples_with_kde(resampled_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6eba5e-d6e9-48fc-956d-694c470b6e5e",
   "metadata": {},
   "source": [
    "In the above, the function `resample`\n",
    "- uses `choice` to uniformly randomly select a choice\n",
    "- from a `range` of integers going from `0` to \n",
    "- the size of the first dimension of the `data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75febee6-6e07-4b1b-8636-d88d241fdb56",
   "metadata": {},
   "source": [
    "Note however that the sampling is without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22285f9a-13a4-4fc0-8d48-b3ab8005f088",
   "metadata": {},
   "source": [
    "**Is it a good idea to sample without replacement?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710fac34-6635-4f55-b9b1-47e91fa017d1",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Sampling without replacement has an important restriction $n'\\leq n$. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247f302-7e15-4182-b6a9-7b8868cabf12",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "without-replacement",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Solution**\n",
    "\n",
    "To allow $n>n'$, at least one sample $(\\R{X}_i,\\R{Y}_i)$ needs to be sampled more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3015b30c-0fd3-4e37-a3bd-7bdf389ed9e1",
   "metadata": {},
   "source": [
    "**Exercise** To allow $n>n'$, complete the following code to sample with replacement and observe what happens when $n \\gg n'$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92dbcc28-c074-431b-8def-77d1b573b5d6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "resample",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3bafad38da4a7a83d39fd5bed38e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1000, continuous_update=False, description='n_', max=10000, min=2), Outp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(n_=widgets.IntSlider(n, 2, 10 * n, continuous_update=False))\n",
    "def plot_resampled_data_with_replacement(n_):\n",
    "    ### BEGIN SOLUTION\n",
    "    XY_ = np.block(\n",
    "        [resample(XY[:, [0]], n_, replace=True), resample(XY[:, [1]], n_, replace=True)]\n",
    "    )\n",
    "    ### END SOLUTION\n",
    "    resampled_data = pd.DataFrame(XY_, columns=[\"X'\", \"Y'\"])\n",
    "    p_ = plot_samples_with_kde(resampled_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcd98e-d07b-426b-9724-52a16e76edf2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**Exercise** \n",
    "\n",
    "Explain whether the resampling trick gives i.i.d. samples $(\\R{X}_{\\R{J}_i},\\R{Y}_{\\R{K}_i})$ for the cases with replacement and without replacement respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b046f-629e-4151-ba09-7dac76de8b7d",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "non-iid",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "**Solution** \n",
    "\n",
    "The samples are identically distributed. However, they are not independent except in the trivial case $n=1$ or $n'=1$, regardless of whether the sample is with replacement or not. Consider $n=1$ and $n'=2$ as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b85aef-f2d2-48a9-a1b8-86276771d84b",
   "metadata": {},
   "source": [
    "### Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963bca6-3230-484e-bca6-15dac88f6352",
   "metadata": {},
   "source": [
    "To improve the stability of the training, MINE applies additional smoothing to the gradient calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915b1f1-5e56-4520-a783-de10ab3da725",
   "metadata": {},
   "source": [
    "**How to train the neural network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc5c71-0973-4e13-9433-639c32a2fabd",
   "metadata": {},
   "source": [
    "{eq}`MINE` can be solved iteratively by minibatch gradient descent using the loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\R{L}_{\\text{MINE}}(\\theta) &:= \\overbrace{- \\frac{1}{\\abs{\\R{B}}} \\sum_{i\\in \\R{B}}  t_{\\theta} (\\R{X}_i, \\R{Y}_i) }^{\\R{L}(\\theta):=} + \\log \\overbrace{\\frac{1}{\\abs{\\R{B}'}} \\sum_{i\\in \\R{B}'}  e^{t_{\\theta} (\\R{X}'_i, \\R{Y}'_i)}}^{\\R{L}'(\\theta):=}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\R{B}$ and $\\R{B}'$ are batches of uniformly randomly chosen indices from $[n]$ and $[n']$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42f67f-4558-450d-9182-3ebca19af445",
   "metadata": {},
   "source": [
    "The gradient expressed in terms of $\\R{L}$ and $\\R{L}'$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\R{L}_{\\text{MINE}}(\\theta) &= \\nabla \\R{L}(\\theta) + \\frac{\\nabla \\R{L}'(\\theta)}{\\R{L}'(\\theta)}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5c8129-33d1-4127-a40c-0592cecb1368",
   "metadata": {},
   "source": [
    "Variation in $\\nabla \\R{L}'(\\theta)$ leads to the variation of the overall gradient especially when $\\R{L}'(\\theta)$ is small. With minibatch gradient descent, the sample average is over a small batch and so the variance can be quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b215572-5cb5-43f8-a3f6-df75965f7407",
   "metadata": {},
   "source": [
    "**How to reduce the variation in the gradient approximation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd6c0a3-b9de-47ff-b2ab-27d7b56fe40b",
   "metadata": {},
   "source": [
    "To alleviate the variation, MINE replaces the denominator $\\R{L}'(\\theta)$ by its moving average:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b409c4-4eab-457b-abf3-1eace29740a0",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{j+1} := \\theta_j - s_j \\underbrace{\\left(\\nabla \\R{L}_j (\\theta_j) + \\frac{\\nabla \\R{L}'_j(\\theta_j)}{\\overline{\\R{L}'}_j}\\right)}_{\\text{(*)}}\n",
    "$$ (MINE:update)\n",
    "\n",
    "where $\\R{L}_j$ and $\\R{L}'_j$ are the \n",
    "\n",
    "$$\n",
    "\\overline{\\R{L}'}_j =  \\beta \\overline{\\R{L}'}_{j-1} + (1-\\beta) \\R{L}'(\\theta_j)\n",
    "$$ (MINE:L2bar)\n",
    "\n",
    "for some smoothing factor $\\beta\\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828190f9-9895-4397-b694-a9fce60f04ee",
   "metadata": {},
   "source": [
    "**How to implement the smoothing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3e59d-034d-4e3b-8e10-1109b3671716",
   "metadata": {},
   "source": [
    "Instead of implementing a new optimizer, a simpler way is to redefine the loss at each step $i$ as follows {cite}`belghazi2018mine`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8dc917-20ad-41cb-9399-ebf7693baa06",
   "metadata": {},
   "source": [
    "$$\n",
    "\\R{L}_{\\text{MINE},j}(\\theta) = \\R{L}_j(\\theta) + \\frac{\\R{L}'_j(\\theta)}{\\overline{\\R{L}'}_j}\n",
    "$$ (MINE:mv)\n",
    "\n",
    "where $\\overline{\\R{L}'}_j$ in {eq}`MINE:L2bar` is regarded as a constant in calculating the gradient. It is easy to verify that $\\nabla \\R{L}_{\\text{MINE},j}(\\theta_j)$ gives the desired gradient (*) in {eq}`MINE:update`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e4b866-fcf9-4799-a513-2537dd0021d7",
   "metadata": {},
   "source": [
    "In summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b6b72-6fca-4084-a7e2-54051b766e46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Definition**\n",
    "\n",
    "MINE estimates the mutual information $I(\\R{X}\\wedge\\R{Y})$ as $\\R{I}_{\\text{MINE}}(\\theta_j)$ {eq}`MINE` where $\\theta_j$ is updated by descending along the gradient of $\\R{L}_{\\text{MINE},j}$ {eq}`MINE:mv` iteratively after $j$ steps.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9135e-85d5-4bf8-b33f-00802377d76c",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Implement a neural network trainer for MINE similar to `DVTrainer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24f5f61-1256-4792-a622-04a35a4c46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9748ba7a-9c9d-4826-a0fb-90e30b6c1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ba4f7-441f-4092-acc9-5ac008aab359",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "net = Net().to(DEVICE)\n",
    "trainer = MINETrainer(Z, Z_ref, net, n_iters_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e0a17-5f5e-4687-ba50-d62325a742a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if input(\"Train? [Y/n]\").lower() != \"n\":\n",
    "    for i in range(10):\n",
    "        trainer.step(10)\n",
    "    plot_net_2(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e9f8ae-93e6-479b-be48-081e66ab6413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDVTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mDVTrainer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    Neural estimator for KL divergence based on the sample DV lower bound.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Estimate D(P_Z||P_Z') using samples Z and Z' by training a network t to maximize\u001b[0m\n",
       "\u001b[0;34m        avg(t(Z)) - log avg(e^t(Z'))\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    parameters:\u001b[0m\n",
       "\u001b[0;34m    ----------\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Z, Z_ref : Tensors with first dimension indicing the samples of Z and Z' respect.\u001b[0m\n",
       "\u001b[0;34m    net : The neural network t that take Z as input and output a real number for each sample.\u001b[0m\n",
       "\u001b[0;34m    n_iters_per_epoch : Number of iterations per epoch.\u001b[0m\n",
       "\u001b[0;34m    writer_params : Parameters to be passed to SummaryWriter for logging.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# constructor\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ_ref\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# set optimizer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# batch sizes\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iters_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m  \u001b[0;31m# ideally a divisor of both n and n'\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# logging\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mwriter_params\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m  \u001b[0;31m# create a new folder under runs/ for logging\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m  \u001b[0;31m# keep counts for logging\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Carries out the gradient descend for a number of epochs and returns \u001b[0m\n",
       "\u001b[0;34m        the divergence estimate evaluated over the entire data.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Loss for each epoch is recorded into the log, but only one divergence \u001b[0m\n",
       "\u001b[0;34m        estimate is computed/logged using the entire dataset. Rerun the method,\u001b[0m\n",
       "\u001b[0;34m        using a loop, to continue to train the neural network and log the result.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Parameters:\u001b[0m\n",
       "\u001b[0;34m        ----------\u001b[0m\n",
       "\u001b[0;34m        epochs : number of epochs\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;31m# random indices for selecting samples for all batches in one epoch\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0midx_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iters_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;31m# obtain a random batch of samples\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbatch_Z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbatch_Z_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0midx_ref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size_ref\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;31m# define the loss as negative DV divergence lower bound\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mDV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_Z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Z_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# calculate gradient\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# descend\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mestimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Estimate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miml/source/part2/dv.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DVTrainer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeed3eeb-532d-4685-82dc-6b0988b9d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "net = Net().to(DEVICE)\n",
    "trainer = MINETrainer(X, Y, net, n_iters_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56f6dd-e301-4c0e-acd2-ed9c17f79d20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MI-NEE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522dede-66d5-44ad-b508-3ac675793d4c",
   "metadata": {},
   "source": [
    "**Is it possible to generate i.i.d. samples for ${\\R{Z}'}^{n'}$?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584a25f-755d-4e98-8307-5cf3c37b7654",
   "metadata": {},
   "source": [
    "Consider another formula for mutual information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc81b16-5238-477c-80b2-48b2c528f4bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Proposition**  \n",
    ":label: MI-3D\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}) - D(P_{\\R{X}}\\|P_{\\R{X}'}) - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\n",
    "\\end{align}\n",
    "$$ (MI-3D)\n",
    "\n",
    "for any product reference distribution $P_{\\R{X}'}\\times P_{\\R{Y}'}$ for which the divergences are finite.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc1af9-d7e8-49f3-9509-ed77a7429f75",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Corollary**  \n",
    ":label: MI-ub\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= \\inf_{\\substack{P_{\\R{X}'}\\in \\mc{P}(\\mc{X})\\\\ P_{\\R{Y}'}\\in \\mc{P}(\\mc{Y})}} D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}).\n",
    "\\end{align}\n",
    "$$ (MI-ub)\n",
    "\n",
    "where the optimal solution is $P_{\\R{X}'}\\times P_{\\R{Y}'}=P_{\\R{X}}\\times P_{\\R{Y}}$, the product of marginal distributions of $\\R{X}$ and $\\R{Y}$. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf194fa-fa9a-4d17-a70f-213c07e4b1f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Proof**\n",
    "\n",
    "{eq}`MI-ub` follows from {eq}`MI-3D` directly since the divergences are non-negative. To prove the proposition:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &= H(\\R{X}) + H(\\R{Y}) - H(\\R{X},\\R{Y})\\\\\n",
    "&= E\\left[-\\log dP_{\\R{X}'}(\\R{X})\\right] - D(P_{\\R{X}}\\|P_{\\R{X}'})\\\\\n",
    "&\\quad+E\\left[-\\log dP_{\\R{Y}'}(\\R{Y})\\right] - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\\\\\n",
    "&\\quad-E\\left[-\\log d(P_{\\R{X}'}\\times P_{\\R{Y}'})(\\R{X},\\R{Y})\\right] + D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'})\\\\\n",
    "&= D(P_{\\R{X},\\R{Y}}\\|P_{\\R{X}'}\\times P_{\\R{Y}'}) - D(P_{\\R{X}}\\|P_{\\R{X}'}) - D(P_{\\R{Y}}\\|P_{\\R{Y}'})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1fecbc-9c51-47c6-b88b-7e1a5adeee5f",
   "metadata": {},
   "source": [
    "*Mutual Information Neural Entropic Estimation (MI-NEE)* {cite}`chan2019neural` uses {eq}`MI-3D` to estimate MI by estimating the three divergences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c956612-7bea-4ddc-a0fc-e36edd3fc439",
   "metadata": {},
   "source": [
    "Applying {eq}`avg-DV` to each divergence in {eq}`MI-3D`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fa4fc-f584-4bda-b570-2e0d5aa1b590",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "I(\\R{X}\\wedge \\R{Y}) &\\approx \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{X},\\R{Y}}(\\R{X}_i,\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{X},\\R{Y}}(\\R{X}'_i,\\R{Y}'_i)}\\\\\n",
    "&\\quad - \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{X}}(\\R{X}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{X}}(\\R{X}'_i)} \\\\\n",
    "&\\quad - \\sup_{t: \\mc{Z} \\to \\mathbb{R}} \\frac1n \\sum_{i\\in [n]} t_{\\R{Y}}(\\R{Y}_i) - \\frac1{n'}\\sum_{i\\in [n']} e^{t_{\\R{Y}}(\\R{Y}'_i)}\n",
    "\\end{align}\n",
    "$$ (MI-NEE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7eb69-7474-4e14-9cfb-d4faea394d7c",
   "metadata": {},
   "source": [
    "$P_{\\R{X}'}$ and $P_{\\R{Y}'}$ are known distributions and so arbitrarily many i.i.d. samples can be drawn from them directly without using the resampling trick {eq}`resample`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4576dba-9848-4689-b84e-59087d83ccd4",
   "metadata": {},
   "source": [
    "Indeed, since the choice of $P_{\\R{X}'}$ and $P_{\\R{Y}'}$ are arbitrary, we can also also train neural networks to optimize them. In particular, {eq}`MI-ub` is a special case where we can train neural networks to approximate $P_{\\R{X}}$ and $P_{\\R{Y}}$."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
